{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas joblib tomli torch torchvision torchaudio tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tomli\n",
    "import joblib\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Config\n",
    "CONFIG_FILE_PATH = \"../config.tomli\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, 'rb') as config_file:\n",
    "    config = tomli.load(config_file)\n",
    "\n",
    "ETF_DATA_DRIVE_PATH = f\"../{config['data']['etfs']}\"\n",
    "STOCK_DATA_DRIVE_PATH = f\"../{config['data']['stocks']}\"\n",
    "PROCESSED_DATA_DRIVE_PATH = f\"../{config['data']['processed']}\"\n",
    "ENGINEERED_DATA_DRIVE_PATH = f\"../{config['data']['engineered']}\"\n",
    "SYMBOLS_FILE_PATH = f\"../{config['data']['symbols']}\"\n",
    "\n",
    "DATASET_PATH = f\"{PROCESSED_DATA_DRIVE_PATH}/dataset.parquet\"\n",
    "ENG_DATASET_PATH = f\"{ENGINEERED_DATA_DRIVE_PATH}/dataset.parquet\"\n",
    "\n",
    "MODEL_DIR = f\"../{config['model']['model_dir']}\"\n",
    "\n",
    "data_dtypes = config['etf_stock_data_type']\n",
    "symbols_dtype = config['symbols_data_types']\n",
    "\n",
    "date_format = config['format']['date_format']\n",
    "\n",
    "rf_model_path = f\"{MODEL_DIR}/{config['model']['rf_model']}\"\n",
    "dl_model_path = f\"{MODEL_DIR}/{config['model']['dl_model']}\"\n",
    "dl_dataset_path = f\"{MODEL_DIR}/{config['model']['dl_dataset_stats']}\"\n",
    "\n",
    "N_JOBS = config['random_forest']['n_jobs']\n",
    "TEST_SIZE = config['random_forest']['test_size']\n",
    "MAX_DEPTH = config['random_forest']['max_depth']\n",
    "N_ESTIMATORS = config['random_forest']['n_estimators']\n",
    "RANDOM_STATE = config['random_forest']['random_state']\n",
    "\n",
    "EPOCHS = config['deep_learning']['epochs']\n",
    "BATCH_SIZE = config['deep_learning']['batch_size']\n",
    "TEST_SPLIT = config['deep_learning']['test_split']\n",
    "RANDOM_STATE = config['deep_learning']['random_state']\n",
    "LEARNING_RATE = config['deep_learning']['learning_rate']\n",
    "NEGATIVE_SLOPE = config['deep_learning']['negative_slope']\n",
    "HIDDEN_LAYER_SIZE = config['deep_learning']['hidden_layer_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger object\n",
    "logger = logging.getLogger('deep_learning')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a file handler and set the logging level\n",
    "file_handler = logging.FileHandler('../logs/deep_learning.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_parquet(ENG_DATASET_PATH)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['vol_moving_avg', 'adj_close_rolling_med']\n",
    "target = 'Volume'\n",
    "\n",
    "# build x, y\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_STATE)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a datasets\n",
    "class DLDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.DataFrame)->None:\n",
    "        # normalize the data  \n",
    "        X = (X - X.mean())/X.std()\n",
    "        y = (y - y.mean())/y.std()\n",
    "\n",
    "        # saving stats\n",
    "        self.X_mean = X.mean()\n",
    "        self.y_mean = y.mean()\n",
    "        self.X_std = X.std()\n",
    "        self.y_std = y.std()\n",
    "\n",
    "        # convert to torch\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "        self.length = len(self.y)\n",
    "    \n",
    "    def __len__(self)->int:\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "train_dataset = DLDataset(X=X_train, y=y_train)\n",
    "dl_dataset_stats = {\n",
    "    'X_mean': train_dataset.X_mean,\n",
    "    'y_mean': train_dataset.y_mean,\n",
    "    'X_std': train_dataset.X_std,\n",
    "    'y_std': train_dataset.y_std,\n",
    "}\n",
    "joblib.dump(dl_dataset_stats, dl_dataset_path)\n",
    "test_dataset = DLDataset(X=X_test, y=y_test)\n",
    "\n",
    "# Creating DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a Deep Learning Model\n",
    "class DLModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, stats:dict=dl_dataset_stats):\n",
    "        super(DLModel, self).__init__()\n",
    "        self.hidden_layer_size = HIDDEN_LAYER_SIZE\n",
    "        self.negative_slope = NEGATIVE_SLOPE\n",
    "        self.stats = stats\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "\n",
    "        self.layers.append(torch.nn.Linear(in_features=2, out_features=self.hidden_layer_size))\n",
    "        self.layers.append(torch.nn.LeakyReLU(negative_slope=self.negative_slope))\n",
    "        # self.layers.append(torch.nn.Linear(in_features=(self.hidden_layer_size * 1), out_features=(self.hidden_layer_size * 2)))\n",
    "        # self.layers.append(torch.nn.LeakyReLU(negative_slope=self.negative_slope))\n",
    "        # self.layers.append(torch.nn.Linear(in_features=(self.hidden_layer_size * 2), out_features=(self.hidden_layer_size * 1)))\n",
    "        # self.layers.append(torch.nn.LeakyReLU(negative_slope=self.negative_slope))\n",
    "        self.layers.append(torch.nn.Linear(in_features=self.hidden_layer_size, out_features=1))\n",
    "        self.layers.append(torch.nn.LeakyReLU(negative_slope=self.negative_slope))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, vol_moving_avg:float, adj_close_rolling_med:float):\n",
    "        self.eval()\n",
    "        # standardizing the data\n",
    "        vol_moving_avg = (vol_moving_avg - self.stats['X_mean']['vol_moving_avg'])/self.stats['X_std']['vol_moving_avg']\n",
    "        adj_close_rolling_med = (adj_close_rolling_med - self.stats['X_mean']['adj_close_rolling_med'])/self.stats['X_std']['adj_close_rolling_med']\n",
    "        # converting to tensor\n",
    "        x = [vol_moving_avg, adj_close_rolling_med]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        # make predictions\n",
    "        y_pred = self.forward(x)\n",
    "        # reverse the y standardization\n",
    "        y_pred = (y_pred * self.stats['y_std']) + self.stats['y_mean']\n",
    "        return y_pred\n",
    "\n",
    "model = DLModel()\n",
    "print(model)\n",
    "\n",
    "# define your loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# define your optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/375 [00:00<?, ?batch/s]c:\\Users\\a_mut\\Documents\\GitHub\\etf-stocks\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32768])) that is different to the input size (torch.Size([32768, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|█████████▉| 374/375 [36:52<00:04,  4.93s/batch]c:\\Users\\a_mut\\Documents\\GitHub\\etf-stocks\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([29764])) that is different to the input size (torch.Size([29764, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 375/375 [36:57<00:00,  5.91s/batch]\n",
      "100%|██████████| 375/375 [30:56<00:00,  4.95s/batch]\n",
      "100%|██████████| 375/375 [34:59<00:00,  5.60s/batch]\n",
      "100%|██████████| 375/375 [47:07<00:00,  7.54s/batch]\n",
      "100%|██████████| 375/375 [44:37<00:00,  7.14s/batch] \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "logger.info(f\"********************* Started training DL Model *********************************\")\n",
    "logger.info(model)\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for X, y in tqdm(train_loader, unit=\"batch\"):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    epoch_loss = total_loss/len(train_loader)\n",
    "    logger.info(f\"Epoch {epoch} loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "for X, y in test_loader:\n",
    "    y_pred = model.forward(X).detach().numpy()\n",
    "    logger.debug(f\"Deep Learning Predictions: {y_pred}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Absolute Error and Mean Squared Error\n",
    "r2 = r2_score(y, y_pred)\n",
    "evs = explained_variance_score(y, y_pred)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "logger.info(f\"Deep Learning Config: {config['deep_learning']}\")\n",
    "logger.info(f\"Deep Learning MAE: {mae}\")\n",
    "logger.info(f\"Deep Learning MSE: {mse}\")\n",
    "logger.info(f\"Deep Learning EVS: {evs}\")\n",
    "logger.info(f\"Deep Learning R^2: {r2}\")\n",
    "logger.info(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.././models/model_deep_learning.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "# a better approach would be MLflow\n",
    "joblib.dump(model, dl_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
